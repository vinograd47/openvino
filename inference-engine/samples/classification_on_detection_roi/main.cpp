// Copyright (C) 2020 Intel Corporation
// SPDX-License-Identifier: Apache-2.0
//

#include "classification_on_detection_roi.h"

#include <samples/common.hpp>
#include <samples/slog.hpp>
#include <samples/args_helper.hpp>

#include <inference_engine.hpp>
#include <vpu/vpu_tools_common.hpp>
#include <format_reader_ptr.h>

#include <gflags/gflags.h>

#include <iostream>
#include <string>
#include <memory>
#include <vector>
#include <algorithm>
#include <map>
#include <atomic>
#include <future>

using namespace InferenceEngine;

bool ParseAndCheckCommandLine(int argc, char *argv[]) {
    gflags::ParseCommandLineNonHelpFlags(&argc, &argv, true);

    if (FLAGS_h) {
        showUsage();
        showAvailableDevices();
        return false;
    }

    slog::info << "Parsing input parameters" << slog::endl;

    if (FLAGS_i.empty()) {
        throw std::logic_error("Parameter -i is not set");
    }
    if (FLAGS_det_model.empty()) {
        throw std::logic_error("Parameter -det_model is not set");
    }
    if (FLAGS_cls_model.empty()) {
        throw std::logic_error("Parameter -cls_model is not set");
    }

    return true;
}

/**
 * \brief The entry point for the Inference Engine classification_on_detection_roi sample application
 * \file classification_on_detection_roi/main.cpp
 * \example classification_on_detection_roi/main.cpp
 */
int main(int argc, char *argv[]) {
    try {
        //
        // Parse and validate input args
        //

        if (!ParseAndCheckCommandLine(argc, argv)) {
            return 0;
        }

        std::vector<std::string> images;
        parseInputFilesArguments(images);

        if (images.empty()) {
            throw std::logic_error("No suitable images were found");
        }

        std::vector<std::string> clsLabels;
        if (!FLAGS_cls_labels.empty()) {
            std::ifstream file(FLAGS_cls_labels);
            if (!file.is_open()) {
                throw std::logic_error("Can't open file with classification labels");
            }

            for (std::string line; std::getline(file, line); ) {
                clsLabels.push_back(std::move(line));
            }
        }

        //
        // Load input images
        //

        slog::info << "Load input images" << slog::endl;

        std::vector<std::shared_ptr<unsigned char>> imagesData;
        std::vector<size_t> imagesWidth;
        std::vector<size_t> imagesHeight;

        imagesData.reserve(images.size());
        imagesWidth.reserve(images.size());
        imagesHeight.reserve(images.size());

        for (const auto& imageFileName : images) {
            FormatReader::ReaderPtr reader(imageFileName.c_str());

            if (reader.get() == nullptr) {
                slog::warn << "    Image " + imageFileName + " cannot be read!" << slog::endl;
                continue;
            }

            imagesData.push_back(reader->getData());
            imagesWidth.push_back(reader->width());
            imagesHeight.push_back(reader->height());
        }

        //
        // Load inference engine
        //

        slog::info << "Load Inference Engine" << slog::endl;

        Core ie;

        slog::info << "    InferenceEngine: " << GetInferenceEngineVersion() << slog::endl;

        slog::info << "    Device info: " << slog::endl;
        slog::info << ie.GetVersions(FLAGS_device) << slog::endl;

        if (!FLAGS_custom_cpu.empty()) {
            // CPU(MKLDNN) extensions are loaded as a shared library and passed as a pointer to base extension

            const auto extension_ptr = make_so_pointer<IExtension>(FLAGS_custom_cpu);
            ie.AddExtension(extension_ptr);

            slog::info << "    CPU Extension loaded: " << FLAGS_custom_cpu << slog::endl;
        }

        if (!FLAGS_custom_cldnn.empty()) {
            // clDNN Extensions are loaded from an .xml description and OpenCL kernel files

            ie.SetConfig({ { PluginConfigParams::KEY_CONFIG_FILE, FLAGS_custom_cldnn } }, "GPU");

            slog::info << "    GPU Extension loaded: " << FLAGS_custom_cldnn << slog::endl;
        }

        //
        // Read IRs generated by ModelOptimizer (.xml and .bin files)
        //

        slog::info << "Loading network IRs:" << slog::endl
            << "\t" << FLAGS_det_model << slog::endl
            << "\t" << FLAGS_cls_model << slog::endl;

        CNNNetwork detNetwork = ie.ReadNetwork(FLAGS_det_model);
        CNNNetwork clsNetwork = ie.ReadNetwork(FLAGS_cls_model);

        //
        // Prepare input info
        //

        slog::info << "Prepare input info" << slog::endl;

        std::string detInputName;
        {
            const auto detInputsInfo = detNetwork.getInputsInfo();

            // SSD network has one input or two inputs (image info)
            if (detInputsInfo.size() != 1) {
                throw std::logic_error("Sample supports detection topologies only with input");
            }

            const auto& item = *detInputsInfo.begin();
            const auto& itemInfo = item.second;
            const auto& itemData = itemInfo->getInputData();
            const auto& itemDims = itemData->getTensorDesc().getDims();

            if (itemDims.size() != 4) {
                throw std::logic_error("Unsupported detection network input");
            }

            detInputName = itemData->getName();
            itemData->setPrecision(Precision::U8);

            itemInfo->getPreProcess().setResizeAlgorithm(RESIZE_AREA);
        }

        std::string clsInputName;
        {
            const auto clsInputsInfo = clsNetwork.getInputsInfo();

            if (clsInputsInfo.size() != 1) {
                throw std::logic_error("Sample supports classification topologies only with 1 input");
            }

            const auto& item = *clsInputsInfo.begin();
            const auto& itemInfo = item.second;
            const auto& itemData = itemInfo->getInputData();
            const auto& itemDims = itemData->getTensorDesc().getDims();

            if (itemDims.size() != 4) {
                throw std::logic_error("Unsupported classification network input");
            }

            clsInputName = itemData->getName();
            itemData->setPrecision(Precision::U8);

            itemInfo->getPreProcess().setResizeAlgorithm(RESIZE_AREA);
        }

        //
        // Prepare output info
        //

        slog::info << "Prepare output info" << slog::endl;

        std::string detOutputName;
        {
            const auto detOutputsInfo = detNetwork.getOutputsInfo();

            if (detOutputsInfo.size() != 1) {
                throw std::logic_error("Sample supports detection topologies only with 1 output");
            }

            const auto& item = *detOutputsInfo.begin();
            const auto& itemData = item.second;
            const auto& itemDims = itemData->getTensorDesc().getDims();

            if (itemDims.size() != 4) {
                throw std::logic_error("Unsupported detection network output");
            }

            const auto objectSize = itemDims[3];

            if (objectSize != 7) {
                throw std::logic_error("Detection output item should have 7 as a last dimension");
            }

            detOutputName = itemData->getName();
            itemData->setPrecision(Precision::FP32);
        }

        std::string clsOutputName;
        {
            const auto clsOutputsInfo = clsNetwork.getOutputsInfo();

            if (clsOutputsInfo.size() != 1) {
                throw std::logic_error("Sample supports classification topologies only with 1 output");
            }

            const auto& item = *clsOutputsInfo.begin();
            const auto& itemData = item.second;
            const auto& itemDims = itemData->getTensorDesc().getDims();

            if (itemDims.size() != 2) {
                throw std::logic_error("Unsupported classificationi network output");
            }
            if (!clsLabels.empty()) {
                if (clsLabels.size() != itemDims[1]) {
                    throw std::logic_error("Number of labels doesn't match classification network output");
                }
            }

            clsOutputName = itemData->getName();
            itemData->setPrecision(Precision::FP32);
        }

        //
        // Load models to the device
        //

        slog::info << "Load models to the device" << slog::endl;

        const auto netConfig = parseConfig(FLAGS_config);

        auto detExeNet = ie.LoadNetwork(detNetwork, FLAGS_device, netConfig);
        auto clsExeNet = ie.LoadNetwork(clsNetwork, FLAGS_device, netConfig);

        //
        // Create infer requests
        //

        slog::info << "Create infer requests" << slog::endl;

        auto detInferRequest = detExeNet.CreateInferRequest();

        const auto numInferRequests =
            clsExeNet.GetMetric(METRIC_KEY(OPTIMAL_NUMBER_OF_INFER_REQUESTS)).as<unsigned int>();

        slog::info << "    Will use " << numInferRequests << " for classification" << slog::endl;

        std::vector<InferRequest> clsInferRequests(numInferRequests);
        for (auto& clsInferRequest : clsInferRequests) {
            clsInferRequest = clsExeNet.CreateInferRequest();
        }

        //
        // Setup detection+classification pipeline
        //

        slog::info << "Setup detection+classification pipeline" << slog::endl;

        const auto finalPostProcessing =
            [&clsLabels](
                    const std::string& imageName,
                    MemoryBlob::Ptr imageBlob,
                    const std::vector<int>& boxes,
                    const std::vector<int>& classes) {
                slog::info << "        Classification results:" << slog::endl;
                for (size_t i = 0; i < classes.size(); i++) {
                    const auto x = boxes.at(i * 4 + 0);
                    const auto y = boxes.at(i * 4 + 1);
                    const auto w = boxes.at(i * 4 + 2);
                    const auto h = boxes.at(i * 4 + 3);

                    slog::info << "            Object at [" << x << ", " << y << " , " << x + w << ", " << y + h << "] : ";
                    if (clsLabels.empty()) {
                        slog::info << "class #" << classes[i];
                    } else {
                        slog::info << clsLabels.at(classes[i]);
                    }
                    slog::info << slog::endl;
                }

                const auto& curImageDims = imageBlob->getTensorDesc().getDims();
                const auto imageWidth = curImageDims[3];
                const auto imageHeight = curImageDims[2];

                const auto imageMem = imageBlob->rwmap();
                const auto imagePtr = imageMem.as<unsigned char*>() +
                    imageBlob->getTensorDesc().getBlockingDesc().getOffsetPadding();

                addRectangles(imagePtr, imageHeight, imageWidth, boxes, classes, BBOX_THICKNESS);

                const std::string image_path = imageName + "_out.bmp";
                slog::info << "        Store result " << image_path << slog::endl;

                writeOutputBmp(image_path, imagePtr, imageHeight, imageWidth);
            };

        const auto clsPostProcessing =
            [&clsOutputName](
                    InferRequest& inferRequest,
                    size_t roiInd,
                    std::vector<int>& classes) {
                const auto outputBlob = as<MemoryBlob>(inferRequest.GetBlob(clsOutputName) );

                const auto& dims = outputBlob->getTensorDesc().getDims();
                const auto numClasses = dims[1];

                const auto outputMem = outputBlob->rmap();
                const auto outputPtr = outputMem.as<const float*>() +
                    outputBlob->getTensorDesc().getBlockingDesc().getOffsetPadding();

                const auto topIter = std::max_element(outputPtr, outputPtr + numClasses);
                const auto topClassInd = std::distance(outputPtr, topIter);

                classes[roiInd] = static_cast<int>(topClassInd);
            };

        const auto runClsInfers =
            [&clsInferRequests, &clsInputName, &clsPostProcessing](
                    const std::vector<Blob::Ptr>& ROIs,
                    std::vector<int>& classes) {
                const auto numRequests = clsInferRequests.size();

                std::vector<size_t> roiInds(numRequests);

                std::atomic<size_t> numStarted(0);
                std::atomic<size_t> numFinished(0);
                std::promise<void> allDone;

                for (size_t inferInd = 0; inferInd < numRequests; ++inferInd) {
                    clsInferRequests[inferInd].SetCompletionCallback(
                        [&clsInferRequests, &clsInputName, &clsPostProcessing,
                         &ROIs, &classes,
                         &roiInds,
                         &numStarted, &numFinished, &allDone,
                         inferInd]() {
                            clsPostProcessing(
                                clsInferRequests[inferInd],
                                roiInds[inferInd],
                                classes);

                            const auto nextRoiInd = numStarted++;
                            const auto finished = ++numFinished;

                            if (nextRoiInd >= ROIs.size()) {
                                if (finished == ROIs.size()) {
                                    allDone.set_value();
                                }
                            } else {
                                slog::info << "Start classification for image #" << inferInd << slog::endl;

                                roiInds[inferInd] = nextRoiInd;
                                clsInferRequests[inferInd].SetBlob(clsInputName, ROIs[nextRoiInd]);
                                clsInferRequests[inferInd].StartAsync();
                            }
                        });
                }

                const auto firstInferBatch = std::min(numRequests, ROIs.size());
                numStarted = firstInferBatch;

                for (size_t inferInd = 0; inferInd < firstInferBatch; ++inferInd) {
                    slog::info << "        Start classification for image #" << inferInd << slog::endl;

                    roiInds[inferInd] = inferInd;
                    clsInferRequests[inferInd].SetBlob(clsInputName, ROIs[inferInd]);
                    clsInferRequests[inferInd].StartAsync();
                }

                auto allDoneFuture = allDone.get_future();
                allDoneFuture.wait();
            };

        const auto getDetectionROIs =
            [](
                    MemoryBlob::Ptr detectionsBlob,
                    Blob::Ptr imageBlob,
                    std::vector<Blob::Ptr>& ROIs,
                    std::vector<int>& boxes) {
                const auto& imageDims = imageBlob->getTensorDesc().getDims();
                const auto imageWidth = imageDims[3];
                const auto imageHeight = imageDims[2];

                const auto& detectionsDims = detectionsBlob->getTensorDesc().getDims();
                const auto maxProposalCount = detectionsDims[2];
                const auto objectSize = detectionsDims[3];

                const auto detectionsMem = detectionsBlob->rmap();
                const auto detectionsPtr = detectionsMem.as<const float*>() +
                    detectionsBlob->getTensorDesc().getBlockingDesc().getOffsetPadding();

                for (size_t curProposal = 0; curProposal < maxProposalCount; curProposal++) {
                    const auto image_id = static_cast<int>(detectionsPtr[curProposal * objectSize + 0]);
                    if (image_id < 0) {
                        break;
                    }

                    const auto xmin = static_cast<size_t>(detectionsPtr[curProposal * objectSize + 3] * imageWidth);
                    const auto ymin = static_cast<size_t>(detectionsPtr[curProposal * objectSize + 4] * imageHeight);
                    const auto xmax = static_cast<size_t>(detectionsPtr[curProposal * objectSize + 5] * imageWidth);
                    const auto ymax = static_cast<size_t>(detectionsPtr[curProposal * objectSize + 6] * imageHeight);

                    ROI roi;
                    roi.id = image_id;
                    roi.posX = xmin;
                    roi.sizeX = xmax - xmin;
                    roi.posY = ymin;
                    roi.sizeY = ymax - ymin;

                    ROIs.push_back(imageBlob->createROI(roi));

                    boxes.push_back(roi.posX);
                    boxes.push_back(roi.posY);
                    boxes.push_back(roi.sizeX);
                    boxes.push_back(roi.sizeY);
                }
            };

        const auto detPostProcessing =
            [&detOutputName, &getDetectionROIs, &runClsInfers, &finalPostProcessing](
                    InferRequest& inferRequest,
                    const std::string& imageName,
                    Blob::Ptr imageBlob) {
                const auto detectionsBlob = as<MemoryBlob>(inferRequest.GetBlob(detOutputName));

                std::vector<Blob::Ptr> ROIs;
                std::vector<int> boxes;
                getDetectionROIs(detectionsBlob, imageBlob, ROIs, boxes);

                slog::info << "        Got " << ROIs.size() << " detections" << slog::endl;

                std::vector<int> classes(ROIs.size());
                runClsInfers(ROIs, classes);

                finalPostProcessing(imageName, as<MemoryBlob>(imageBlob), boxes, classes);
            };

        //
        // Run detection+classification pipeline
        //

        slog::info << "Run detection+classification pipeline" << slog::endl;

        const auto createImageBlob =
            [&imagesData, &imagesWidth, &imagesHeight](size_t imageInd) -> MemoryBlob::Ptr {
                const auto imageDataPtr = imagesData[imageInd].get();

                const auto imageWidth = imagesWidth[imageInd];
                const auto imageHeight = imagesHeight[imageInd];

                const auto blobDims = SizeVector {1, 3, imageHeight, imageWidth};
                const auto blobDesc = TensorDesc(Precision::U8, blobDims, Layout::NHWC);

                return make_shared_blob<uint8_t>(blobDesc, imageDataPtr);
            };

        for (size_t imageInd = 0; imageInd < images.size(); ++imageInd) {
            const auto& imageName = images[imageInd];
            const auto imageBlob = createImageBlob(imageInd);

            slog::info << "    Process " << imageName << slog::endl;

            detInferRequest.SetBlob(detInputName, imageBlob);
            detInferRequest.Infer();

            detPostProcessing(detInferRequest, imageName, imageBlob);
        }
    }
    catch (const std::exception& error) {
        slog::err << error.what() << slog::endl;
        return 1;
    }
    catch (...) {
        slog::err << "Unknown/internal exception happened." << slog::endl;
        return 1;
    }

    slog::info << "Execution successful" << slog::endl;
    slog::info << slog::endl << "This sample is an API example, for any performance measurements "
                                "please use the dedicated benchmark_app tool" << slog::endl;

    return 0;
}
